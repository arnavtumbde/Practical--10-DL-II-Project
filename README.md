
---

# Probabilistic Generative Modeling with Variational Autoencoders and Normalizing Flows

## Arnav Tumbde Batch : A2 | Roll no : 30 

**Probabilistic Generative Modeling using TensorFlow Probability**

---

## Problem Statement

In real-world datasets, uncertainty and variability are inherent. Traditional deterministic neural networks fail to model this uncertainty, which can lead to overconfident predictions. This project explores **probabilistic deep learning** by generating and modeling complex distributions using **Normalizing Flows** and **Variational Autoencoders (VAEs)**.

The objective is to:

* Generate synthetic data from a transformed base distribution.
* Train a VAE to reconstruct and generate new samples.
* Visualize latent embeddings and understand uncertainty in data generation.

This addresses challenges in generative modeling, uncertainty quantification, and probabilistic reasoning in deep learning.

---

## Explanation

The project pipeline consists of the following steps:

1. **Base Distribution Creation**

   * A 2D Gaussian distribution is used as the starting point.

2. **Normalizing Flow for Data Transformation**

   * Bijectors such as `Polynomial`, `Rotation`, `Shift`, `Scale`, and `Tanh` are chained to transform the base distribution into complex distributions.
   * These transformed distributions are visualized as scatter plots and contour images.

3. **Image Dataset Generation**

   * Using the normalizing flow, a dataset of contour images (36x36x3) is generated.
   * At least 2,500 images are created to train the VAE.

4. **TensorFlow Dataset Preparation**

   * Dataset is split into training and validation sets using `tf.data.Dataset`.
   * Images are normalized and batched for training.

5. **Variational Autoencoder (VAE)**

   * **Encoder**: Convolutional network with probabilistic layers to output a latent distribution.
   * **Decoder**: Convolutional network generating images from latent vectors.
   * KL divergence loss is used to regularize the latent space.
   * VAE is trained using the negative log-likelihood loss.

6. **Evaluation and Visualization**

   * Reconstructed images are compared with originals.
   * Latent embeddings are visualized in 2D scatter plots.
   * New images can be generated by sampling from the prior latent distribution.
   * Optional animation demonstrates latent space interpolation.

---

## Techniques Used

* **Normalizing Flows**: Transform base distributions to complex distributions.
* **Variational Autoencoders (VAEs)**: Probabilistic generative model for image synthesis.
* **Bayesian Neural Networks**: Probabilistic layers with KL divergence for uncertainty quantification.
* **TensorFlow Probability (TFP)**: Framework for implementing probabilistic deep learning models.

---

## Dataset

* The dataset is **synthetically generated** using normalizing flows within the notebook.
* Each image is a 36x36 RGB contour plot representing a sample from a transformed distribution.
* Total images: 2,500.
* Format: `NumPy array` of shape `(2500, 36, 36, 3)`.
* **Note**: No external dataset is required; the dataset generation code is included in the notebook.

---

## Dependencies

* Python 3.8+
* TensorFlow 2.x
* TensorFlow Probability
* NumPy
* Matplotlib

---

## How to Run

1. Clone the repository:

```bash
git clone https://github.com/mohd-faizy/Probabilistic-Deep-Learning-with-TensorFlow.git
cd Probabilistic-Deep-Learning-with-TensorFlow/05_Capstone_Project
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Open the notebook in **Google Colab** or **Jupyter Notebook**.
4. Run cells sequentially to generate dataset, train the VAE, and visualize outputs.

---

## Outputs

* Scatter plots of base and transformed distributions.
* Contour plot images of the generated dataset.
* Training and validation loss curves.
* Reconstructed images from the VAE.
* Latent space embeddings and new generated images.
* Optional animation of latent space interpolation.

---

## References

* TensorFlow Probability Guide: [https://www.tensorflow.org/probability](https://www.tensorflow.org/probability)
* Kingma & Welling, Auto-Encoding Variational Bayes (2013)
* Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models (2014)
* Blundell et al., Weight Uncertainty in Neural Networks (2015)

---
